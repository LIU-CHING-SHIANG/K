{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c48b3ee-1c67-4afe-8de9-51e1a7b04565",
   "metadata": {},
   "source": [
    "90天波動係數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4bb4529-bf91-4b7d-8449-685596e1f423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Date  Ticker  Adj_Close  90_day_std  90_day_avg     90DCV\n",
      "93787 2024-10-04    6505       54.5    5.665928   60.189778  0.094134\n",
      "93788 2024-10-07    6505       55.0    5.655180   60.065111  0.094151\n",
      "93789 2024-10-08    6505       54.2    5.655891   59.936889  0.094364\n",
      "93790 2024-10-09    6505       52.3    5.660582   59.772556  0.094702\n",
      "93791 2024-10-11    6505       52.0    5.678327   59.615667  0.095249\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('50合併.csv')\n",
    "# 假设df是您的DataFrame\n",
    "# 将日期转换为datetime格式\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# 将'Close'列转换为数值型\n",
    "df['Adj_Close'] = pd.to_numeric(df['Adj_Close'], errors='coerce')\n",
    "\n",
    "# 删除包含NaN的行，或者选择适合您的数据处理方式\n",
    "df = df.dropna(subset=['Adj_Close'])\n",
    "\n",
    "# 按照'ticker'和'Date'排序\n",
    "df.sort_values(['Ticker', 'Date'], inplace=True)\n",
    "\n",
    "# 对每个'ticker'进行滚动计算\n",
    "df['90_day_std'] = df.groupby('Ticker')['Adj_Close'].rolling(window=90).std().reset_index(level=0, drop=True)\n",
    "df['90_day_avg'] = df.groupby('Ticker')['Adj_Close'].rolling(window=90).mean().reset_index(level=0, drop=True)\n",
    "\n",
    "# 计算90天波动系数\n",
    "df['90DCV'] = df['90_day_std'] / df['90_day_avg']\n",
    "\n",
    "# 输出结果\n",
    "print(df[['Date', 'Ticker', 'Adj_Close', '90_day_std', '90_day_avg', '90DCV']].tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ad33a4c-df7a-4f20-baa1-bb2b9ac57888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择要导出的列\n",
    "export_columns = ['Date', 'Ticker', '90DCV']\n",
    "\n",
    "# 导出到 CSV 文件\n",
    "df[export_columns].to_csv('90DCV_output.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d1b352-2e28-4afd-aa33-66227002484c",
   "metadata": {},
   "source": [
    "Beta係數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b72ffea8-b67d-42ab-83ad-1c95c061dadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已实现波动率结果已成功导出到 'Realized_Vol_results.csv'。\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. 数据加载\n",
    "df_stocks = pd.read_csv('50合併.csv')\n",
    "\n",
    "# 2. 数据预处理\n",
    "\n",
    "# 转换 'Date' 列为 datetime 格式\n",
    "df_stocks['Date'] = pd.to_datetime(df_stocks['Date'], format='%Y/%m/%d')\n",
    "\n",
    "# 确保 'ticker' 列为字符串类型\n",
    "df_stocks['ticker'] = df_stocks['ticker'].astype(str)\n",
    "\n",
    "# 转换 'Adj_Close' 列为数值型，无法转换的值设为 NaN\n",
    "df_stocks['Adj_Close'] = pd.to_numeric(df_stocks['Adj_Close'], errors='coerce')\n",
    "\n",
    "# 按照 'ticker' 和 'Date' 排序\n",
    "df_stocks.sort_values(['ticker', 'Date'], inplace=True)\n",
    "\n",
    "# 计算每日的对数回报率，使用 transform 保持索引对齐\n",
    "df_stocks['Log_Return'] = df_stocks.groupby('ticker')['Adj_Close'].transform(\n",
    "    lambda x: np.log(x / x.shift(1))\n",
    ")\n",
    "\n",
    "# 定义滚动窗口的大小（21 天）\n",
    "window_size = 21\n",
    "\n",
    "# 计算已实现波动率（每个 ticker 分组计算），使用 transform 保持索引对齐\n",
    "df_stocks['Realized_Vol'] = df_stocks.groupby('ticker')['Log_Return'].transform(\n",
    "    lambda x: np.sqrt(252) * np.sqrt((x**2).rolling(window=window_size).mean())\n",
    ")\n",
    "\n",
    "# 移除包含 NaN 的行（由于滚动窗口导致的前 window-1 天数据缺失）\n",
    "df_stocks = df_stocks.dropna(subset=['Realized_Vol'])\n",
    "\n",
    "# 3. 导出结果\n",
    "\n",
    "# 选择要导出的列\n",
    "export_columns = ['Date', 'ticker', 'Adj_Close', 'Log_Return', 'Realized_Vol']\n",
    "\n",
    "# 检查 export_columns 中的列是否存在于 df_stocks 中\n",
    "missing_cols = [col for col in export_columns if col not in df_stocks.columns]\n",
    "if missing_cols:\n",
    "    print(f\"以下列不存在于 DataFrame 中，无法导出: {missing_cols}\")\n",
    "else:\n",
    "    # 导出为 CSV 文件\n",
    "    df_stocks[export_columns].to_csv('Realized_Vol_results.csv', index=False)\n",
    "    print(\"已实现波动率结果已成功导出到 'Realized_Vol_results.csv'。\")\n",
    "    \n",
    "    # 如果需要导出为 Excel 文件，请取消以下注释\n",
    "    # df_stocks[export_columns].to_excel('Realized_Vol_results.xlsx', index=False)\n",
    "    # print(\"已实现波动率结果已成功导出到 'Realized_Vol_results.xlsx'。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2f42c9-cbf4-4786-80f6-ea1a3fb0756c",
   "metadata": {},
   "source": [
    "一個月實現波動率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c16be3d6-886f-4f75-b16d-6d2c46f96f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asas9\\AppData\\Local\\Temp\\ipykernel_28296\\3511032184.py:6: DtypeWarning: Columns (2,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\asas9\\AppData\\Local\\Temp\\ipykernel_28296\\3511032184.py:16: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.set_index('Date').groupby('ticker', group_keys=False).apply(lambda x: x.asfreq('B')).reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Date  ticker  Adj_Close  log_return  AnnVol1M\n",
      "21     2016-11-09  1101.0      18.92   -0.032244  0.263921\n",
      "22     2016-11-10  1101.0      19.88    0.049495  0.314723\n",
      "23     2016-11-11  1101.0      19.54   -0.017251  0.303366\n",
      "24     2016-11-14  1101.0      19.31   -0.011841  0.294780\n",
      "25     2016-11-15  1101.0      19.13   -0.009365  0.295862\n",
      "...           ...     ...        ...         ...       ...\n",
      "100266 2024-10-04  6505.0      54.50    0.041204  0.408853\n",
      "100267 2024-10-07  6505.0      55.00    0.009132  0.409639\n",
      "100268 2024-10-08  6505.0      54.20   -0.014652  0.356282\n",
      "100269 2024-10-09  6505.0      52.30   -0.035685  0.366612\n",
      "100271 2024-10-11  6505.0      52.00   -0.005753  0.364849\n",
      "\n",
      "[92401 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the uploaded CSV file\n",
    "file_path = '50合併.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert Date column to datetime format and sort by date and ticker\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%Y/%m/%d')\n",
    "df = df.sort_values(by=['ticker', 'Date'])\n",
    "\n",
    "# Convert relevant columns to numeric types to avoid type errors\n",
    "df['Adj_Close'] = pd.to_numeric(df['Adj_Close'], errors='coerce')\n",
    "\n",
    "# Ensure each group has a continuous business day index\n",
    "df = df.set_index('Date').groupby('ticker', group_keys=False).apply(lambda x: x.asfreq('B')).reset_index()\n",
    "\n",
    "# Calculate daily log returns using a loop to avoid index issues\n",
    "log_return_list = []\n",
    "for name, group in df.groupby('ticker'):\n",
    "    group['log_return'] = np.log(group['Adj_Close'] / group['Adj_Close'].shift(1))\n",
    "    log_return_list.append(group)\n",
    "\n",
    "df = pd.concat(log_return_list)\n",
    "\n",
    "# Calculate realized annualized volatility for the past month (approx. 21 trading days)\n",
    "vol_list = []\n",
    "for name, group in df.groupby('ticker'):\n",
    "    group['AnnVol1M'] = group['log_return'].rolling(window=21).apply(lambda x: np.sqrt(252 * np.sum(x**2) / len(x)))\n",
    "    vol_list.append(group)\n",
    "\n",
    "df = pd.concat(vol_list)\n",
    "\n",
    "# Filter the DataFrame to remove rows with missing values in the calculated columns for AnnVol1M\n",
    "df_filtered_annvol1m = df[['Date', 'ticker', 'Adj_Close', 'log_return', 'AnnVol1M']].dropna()\n",
    "\n",
    "# Display the filtered results for AnnVol1M\n",
    "print(df_filtered_annvol1m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "682e41bc-0fed-4544-8593-52440c91b834",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_columns = ['Date', 'ticker', 'AnnVol1M']\n",
    "# 导出到 CSV 文件\n",
    "df_filtered_annvol1m[export_columns].to_csv('AnnVol1M.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdb5fbc-4ec6-49d1-a776-588df8494f53",
   "metadata": {},
   "source": [
    "12個月年化波動率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ac650ed0-35e7-490e-ad48-66c1eb91fb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asas9\\AppData\\Local\\Temp\\ipykernel_28296\\804330945.py:6: DtypeWarning: Columns (2,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\asas9\\AppData\\Local\\Temp\\ipykernel_28296\\804330945.py:16: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.set_index('Date').groupby('ticker', group_keys=False).apply(lambda x: x.asfreq('B')).reset_index()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Date  ticker  Adj_Close  log_return  AnnVol12M\n",
      "269    2017-10-23  1101.0      18.35   -0.003264   0.178680\n",
      "270    2017-10-24  1101.0      18.22   -0.007110   0.178821\n",
      "271    2017-10-25  1101.0      18.19   -0.001648   0.176344\n",
      "272    2017-10-26  1101.0      18.16   -0.001651   0.174733\n",
      "273    2017-10-27  1101.0      18.16    0.000000   0.174635\n",
      "...           ...     ...        ...         ...        ...\n",
      "100266 2024-10-04  6505.0      54.50    0.041204   0.229437\n",
      "100267 2024-10-07  6505.0      55.00    0.009132   0.229590\n",
      "100268 2024-10-08  6505.0      54.20   -0.014652   0.229299\n",
      "100269 2024-10-09  6505.0      52.30   -0.035685   0.232027\n",
      "100271 2024-10-11  6505.0      52.00   -0.005753   0.232086\n",
      "\n",
      "[80336 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the uploaded CSV file\n",
    "file_path = '50合併.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert Date column to datetime format and sort by date and ticker\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%Y/%m/%d')\n",
    "df = df.sort_values(by=['ticker', 'Date'])\n",
    "\n",
    "# Convert relevant columns to numeric types to avoid type errors\n",
    "df['Adj_Close'] = pd.to_numeric(df['Adj_Close'], errors='coerce')\n",
    "\n",
    "# Ensure each group has a continuous business day index\n",
    "df = df.set_index('Date').groupby('ticker', group_keys=False).apply(lambda x: x.asfreq('B')).reset_index()\n",
    "\n",
    "# Calculate daily log returns using a loop to avoid index issues\n",
    "log_return_list = []\n",
    "for name, group in df.groupby('ticker'):\n",
    "    group['log_return'] = np.log(group['Adj_Close'] / group['Adj_Close'].shift(1))\n",
    "    log_return_list.append(group)\n",
    "\n",
    "df = pd.concat(log_return_list)\n",
    "\n",
    "# Calculate realized annualized volatility for the past year (approx. 252 trading days)\n",
    "df['AnnVol12M'] = df.groupby('ticker')['log_return'].rolling(window=252).apply(lambda x: np.sqrt(252 * np.sum(x**2) / len(x))).reset_index(level=0, drop=True)\n",
    "\n",
    "# Filter the DataFrame to remove rows with missing values in the calculated columns for AnnVol12M\n",
    "df_filtered_annvol12m = df[['Date', 'ticker', 'Adj_Close', 'log_return', 'AnnVol12M']].dropna()\n",
    "\n",
    "# Display the filtered results for AnnVol12M\n",
    "print(df_filtered_annvol12m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ebf16fe2-91b0-496f-95ff-7a32c9c14633",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_columns = ['Date', 'ticker', 'AnnVol12M']\n",
    "# 导出到 CSV 文件\n",
    "df_filtered_annvol12m[export_columns].to_csv('AnnVol12M.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4c2538-cb9a-424d-b7d8-80a49e46ff93",
   "metadata": {},
   "source": [
    "銷售對企業價值比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1924e90a-2b85-4291-bc75-fa352f63399a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asas9\\AppData\\Local\\Temp\\ipykernel_7732\\3647532618.py:5: DtypeWarning: Columns (2,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\asas9\\AppData\\Local\\Temp\\ipykernel_7732\\3647532618.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_cleaned = df_cleaned.set_index('Date').groupby('ticker', group_keys=False).apply(lambda x: x.asfreq('B')).reset_index()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the uploaded CSV file\n",
    "file_path = '50合併.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert Date column to datetime format and sort by date and ticker\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%Y/%m/%d')\n",
    "df = df.sort_values(by=['ticker', 'Date'])\n",
    "\n",
    "# Convert relevant columns to numeric types to avoid type errors\n",
    "df['SALEQ'] = pd.to_numeric(df['SALEQ'], errors='coerce')\n",
    "df['CSHOQ'] = pd.to_numeric(df['CSHOQ'], errors='coerce')\n",
    "df['Adj_Close'] = pd.to_numeric(df['Adj_Close'], errors='coerce')\n",
    "df['DLTTQ'] = pd.to_numeric(df['DLTTQ'], errors='coerce')\n",
    "df['DLCQ'] = pd.to_numeric(df['DLCQ'], errors='coerce')\n",
    "df['PSTKQ'] = pd.to_numeric(df['PSTKQ'], errors='coerce')\n",
    "df['MIBTQ'] = pd.to_numeric(df['MIBTQ'], errors='coerce')\n",
    "df['CHEQ'] = pd.to_numeric(df['CHEQ'], errors='coerce')\n",
    "\n",
    "# Remove rows with missing values initially to ensure alignment is correct\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "# Set Date as the index and ensure continuity for each ticker by reindexing to include all business days\n",
    "df_cleaned = df_cleaned.set_index('Date').groupby('ticker', group_keys=False).apply(lambda x: x.asfreq('B')).reset_index()\n",
    "\n",
    "# Recalculate rolling metrics using a rolling window of approximately 252 trading days (to approximate 4 quarters), with min_periods set to 252\n",
    "df_cleaned['Total_Sales_4_Quarters_Rolling'] = df_cleaned.groupby('ticker')['SALEQ'].rolling(window=252, min_periods=252).sum().reset_index(level=0, drop=True)\n",
    "df_cleaned['Avg_CSHOQ_4_Quarters_Rolling'] = df_cleaned.groupby('ticker')['CSHOQ'].rolling(window=252, min_periods=252).mean().reset_index(level=0, drop=True)\n",
    "\n",
    "# Recalculate the enterprise value (EV) using rolling 4-quarter data, rolling daily\n",
    "df_cleaned['EV_4_Quarters_Rolling'] = (df_cleaned['Avg_CSHOQ_4_Quarters_Rolling'] * df_cleaned['Adj_Close']) + df_cleaned['DLTTQ'] + df_cleaned['DLCQ'] + df_cleaned['PSTKQ'] + df_cleaned['MIBTQ'] - df_cleaned['CHEQ']\n",
    "\n",
    "# Recalculate the sales-to-enterprise value ratio (SEV) using rolling 4-quarter data, rolling daily\n",
    "df_cleaned['SEV_4_Quarters_Rolling'] = df_cleaned['Total_Sales_4_Quarters_Rolling'] / df_cleaned['EV_4_Quarters_Rolling']\n",
    "\n",
    "# Filter the DataFrame to remove rows with missing values in the calculated columns\n",
    "df_filtered_cleaned = df_cleaned[['Date', 'ticker', 'Total_Sales_4_Quarters_Rolling', 'EV_4_Quarters_Rolling', 'SEV_4_Quarters_Rolling']].dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef1b9ce3-0666-4661-9bc7-d255e35c4203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>ticker</th>\n",
       "      <th>Total_Sales_4_Quarters_Rolling</th>\n",
       "      <th>EV_4_Quarters_Rolling</th>\n",
       "      <th>SEV_4_Quarters_Rolling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>2017-10-20</td>\n",
       "      <td>1101.0</td>\n",
       "      <td>5.801347e+12</td>\n",
       "      <td>1.984236e+11</td>\n",
       "      <td>29.237173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>2017-10-23</td>\n",
       "      <td>1101.0</td>\n",
       "      <td>5.803119e+12</td>\n",
       "      <td>1.982021e+11</td>\n",
       "      <td>29.278792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>2017-10-24</td>\n",
       "      <td>1101.0</td>\n",
       "      <td>5.804890e+12</td>\n",
       "      <td>1.977221e+11</td>\n",
       "      <td>29.358830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>2017-10-25</td>\n",
       "      <td>1101.0</td>\n",
       "      <td>5.806662e+12</td>\n",
       "      <td>1.976114e+11</td>\n",
       "      <td>29.384253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>2017-10-26</td>\n",
       "      <td>1101.0</td>\n",
       "      <td>5.808434e+12</td>\n",
       "      <td>1.975006e+11</td>\n",
       "      <td>29.409704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100266</th>\n",
       "      <td>2024-10-04</td>\n",
       "      <td>6505.0</td>\n",
       "      <td>4.476381e+13</td>\n",
       "      <td>5.958281e+11</td>\n",
       "      <td>75.128734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100267</th>\n",
       "      <td>2024-10-07</td>\n",
       "      <td>6505.0</td>\n",
       "      <td>4.477503e+13</td>\n",
       "      <td>6.005911e+11</td>\n",
       "      <td>74.551607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100268</th>\n",
       "      <td>2024-10-08</td>\n",
       "      <td>6505.0</td>\n",
       "      <td>4.478625e+13</td>\n",
       "      <td>5.929703e+11</td>\n",
       "      <td>75.528653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100269</th>\n",
       "      <td>2024-10-09</td>\n",
       "      <td>6505.0</td>\n",
       "      <td>4.479747e+13</td>\n",
       "      <td>5.748710e+11</td>\n",
       "      <td>77.926125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100271</th>\n",
       "      <td>2024-10-11</td>\n",
       "      <td>6505.0</td>\n",
       "      <td>4.480869e+13</td>\n",
       "      <td>5.720132e+11</td>\n",
       "      <td>78.335058</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81466 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date  ticker  Total_Sales_4_Quarters_Rolling  \\\n",
       "268    2017-10-20  1101.0                    5.801347e+12   \n",
       "269    2017-10-23  1101.0                    5.803119e+12   \n",
       "270    2017-10-24  1101.0                    5.804890e+12   \n",
       "271    2017-10-25  1101.0                    5.806662e+12   \n",
       "272    2017-10-26  1101.0                    5.808434e+12   \n",
       "...           ...     ...                             ...   \n",
       "100266 2024-10-04  6505.0                    4.476381e+13   \n",
       "100267 2024-10-07  6505.0                    4.477503e+13   \n",
       "100268 2024-10-08  6505.0                    4.478625e+13   \n",
       "100269 2024-10-09  6505.0                    4.479747e+13   \n",
       "100271 2024-10-11  6505.0                    4.480869e+13   \n",
       "\n",
       "        EV_4_Quarters_Rolling  SEV_4_Quarters_Rolling  \n",
       "268              1.984236e+11               29.237173  \n",
       "269              1.982021e+11               29.278792  \n",
       "270              1.977221e+11               29.358830  \n",
       "271              1.976114e+11               29.384253  \n",
       "272              1.975006e+11               29.409704  \n",
       "...                       ...                     ...  \n",
       "100266           5.958281e+11               75.128734  \n",
       "100267           6.005911e+11               74.551607  \n",
       "100268           5.929703e+11               75.528653  \n",
       "100269           5.748710e+11               77.926125  \n",
       "100271           5.720132e+11               78.335058  \n",
       "\n",
       "[81466 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9c6e4bb-099a-4ae3-9c27-6ad9ca2610ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_columns = ['Date', 'ticker', 'SEV_4_Quarters_Rolling']\n",
    "# 导出到 CSV 文件\n",
    "df_filtered_cleaned[export_columns].to_csv('SEV.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69163583-3205-4f8d-8194-55be3a7cfe0c",
   "metadata": {},
   "source": [
    "****股息殖利率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e16f6dc-9feb-4700-b31b-bfdfb881dae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Date  ticker  Total_DVQ_4_Quarters_Rolling  \\\n",
      "268    2017-10-20  1101.0                           0.0   \n",
      "269    2017-10-23  1101.0                           0.0   \n",
      "270    2017-10-24  1101.0                           0.0   \n",
      "271    2017-10-25  1101.0                           0.0   \n",
      "272    2017-10-26  1101.0                           0.0   \n",
      "...           ...     ...                           ...   \n",
      "100266 2024-10-04  6505.0                           0.0   \n",
      "100267 2024-10-07  6505.0                           0.0   \n",
      "100268 2024-10-08  6505.0                           0.0   \n",
      "100269 2024-10-09  6505.0                           0.0   \n",
      "100271 2024-10-11  6505.0                           0.0   \n",
      "\n",
      "        Avg_CSHOQ_4_Quarters_Rolling  Adj_Close  DivP  \n",
      "268                     3.692176e+09      18.41   0.0  \n",
      "269                     3.692176e+09      18.35   0.0  \n",
      "270                     3.692176e+09      18.22   0.0  \n",
      "271                     3.692176e+09      18.19   0.0  \n",
      "272                     3.692176e+09      18.16   0.0  \n",
      "...                              ...        ...   ...  \n",
      "100266                  9.525960e+09      54.50   0.0  \n",
      "100267                  9.525960e+09      55.00   0.0  \n",
      "100268                  9.525960e+09      54.20   0.0  \n",
      "100269                  9.525960e+09      52.30   0.0  \n",
      "100271                  9.525960e+09      52.00   0.0  \n",
      "\n",
      "[81484 rows x 6 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asas9\\AppData\\Local\\Temp\\ipykernel_28296\\2905873931.py:17: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.set_index('Date').groupby('ticker', group_keys=False).apply(lambda x: x.asfreq('B')).reset_index()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the uploaded CSV file\n",
    "file_path = '50合併.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert Date column to datetime format and sort by date and ticker\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%Y/%m/%d')\n",
    "df = df.sort_values(by=['ticker', 'Date'])\n",
    "\n",
    "# Convert relevant columns to numeric types to avoid type errors\n",
    "df['DVQ'] = pd.to_numeric(df['DVQ'], errors='coerce')\n",
    "df['CSHOQ'] = pd.to_numeric(df['CSHOQ'], errors='coerce')\n",
    "df['Adj_Close'] = pd.to_numeric(df['Adj_Close'], errors='coerce')\n",
    "\n",
    "# Ensure each group has a continuous business day index\n",
    "df = df.set_index('Date').groupby('ticker', group_keys=False).apply(lambda x: x.asfreq('B')).reset_index()\n",
    "\n",
    "# Calculate rolling sum of DVQ over the past 4 quarters (approx. 252 trading days)\n",
    "df['Total_DVQ_4_Quarters_Rolling'] = df.groupby('ticker')['DVQ'].rolling(window=252, min_periods=252).sum().reset_index(level=0, drop=True)\n",
    "\n",
    "# Calculate rolling average of CSHOQ over the past 4 quarters (approx. 252 trading days)\n",
    "df['Avg_CSHOQ_4_Quarters_Rolling'] = df.groupby('ticker')['CSHOQ'].rolling(window=252, min_periods=252).mean().reset_index(level=0, drop=True)\n",
    "\n",
    "# Calculate dividend yield (DivP) based on the provided formula\n",
    "df['DivP'] = df['Total_DVQ_4_Quarters_Rolling'] / (df['Adj_Close'] * df['Avg_CSHOQ_4_Quarters_Rolling'])\n",
    "\n",
    "# Filter the DataFrame to remove rows with missing values in the calculated columns\n",
    "df_filtered = df[['Date', 'ticker', 'Total_DVQ_4_Quarters_Rolling', 'Avg_CSHOQ_4_Quarters_Rolling', 'Adj_Close', 'DivP']].dropna()\n",
    "\n",
    "# Display the filtered results\n",
    "print(df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a3a7ca99-b896-499f-9f33-fa51db4edd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_columns = ['Date', 'ticker', 'DivP']\n",
    "# 导出到 CSV 文件\n",
    "df_filtered[export_columns].to_csv('DivP.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998e1426-7ad3-4942-86b8-815f5706d2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "每股收益殖利率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "28963873-7a8a-466b-bd33-8bcaf6f3c504",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asas9\\AppData\\Local\\Temp\\ipykernel_28296\\2656754767.py:5: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Date  ticker  Total_EPS_4_Quarters_Rolling  Adj_Close        EP\n",
      "268    2017-10-20  1101.0                        125.13      18.41  6.796850\n",
      "269    2017-10-23  1101.0                        125.17      18.35  6.821253\n",
      "270    2017-10-24  1101.0                        125.21      18.22  6.872119\n",
      "271    2017-10-25  1101.0                        125.25      18.19  6.885651\n",
      "272    2017-10-26  1101.0                        125.29      18.16  6.899229\n",
      "...           ...     ...                           ...        ...       ...\n",
      "100266 2024-10-04  6505.0                        205.04      54.50  3.762202\n",
      "100267 2024-10-07  6505.0                        205.45      55.00  3.735455\n",
      "100268 2024-10-08  6505.0                        205.86      54.20  3.798155\n",
      "100269 2024-10-09  6505.0                        206.27      52.30  3.943977\n",
      "100271 2024-10-11  6505.0                        206.68      52.00  3.974615\n",
      "\n",
      "[80116 rows x 5 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asas9\\AppData\\Local\\Temp\\ipykernel_28296\\2656754767.py:16: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.set_index('Date').groupby('ticker', group_keys=False).apply(lambda x: x.asfreq('B')).reset_index()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the uploaded CSV file\n",
    "file_path = '50合併.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert Date column to datetime format and sort by date and ticker\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%Y/%m/%d')\n",
    "df = df.sort_values(by=['ticker', 'Date'])\n",
    "\n",
    "# Convert relevant columns to numeric types to avoid type errors\n",
    "df['EPS'] = pd.to_numeric(df['EPS'], errors='coerce')\n",
    "df['Adj_Close'] = pd.to_numeric(df['Adj_Close'], errors='coerce')\n",
    "\n",
    "# Ensure each group has a continuous business day index\n",
    "df = df.set_index('Date').groupby('ticker', group_keys=False).apply(lambda x: x.asfreq('B')).reset_index()\n",
    "\n",
    "# Calculate rolling sum of EPS over the past 4 quarters (approx. 252 trading days)\n",
    "df['Total_EPS_4_Quarters_Rolling'] = df.groupby('ticker')['EPS'].rolling(window=252, min_periods=252).sum().reset_index(level=0, drop=True)\n",
    "\n",
    "# Calculate earnings yield (EP) based on the provided formula\n",
    "df['EP'] = df['Total_EPS_4_Quarters_Rolling'] / df['Adj_Close']\n",
    "\n",
    "# Filter the DataFrame to remove rows with missing values in the calculated columns\n",
    "df_filtered = df[['Date', 'ticker', 'Total_EPS_4_Quarters_Rolling', 'Adj_Close', 'EP']].dropna()\n",
    "\n",
    "# Display the filtered results\n",
    "print(df_filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "41a4a60a-c683-43a2-9ae2-ceaf054e67cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_columns = ['Date', 'ticker', 'EP']\n",
    "# 导出到 CSV 文件\n",
    "df_filtered[export_columns].to_csv('EP.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b989c34f-06f2-4a47-a442-596ee4cb1b17",
   "metadata": {},
   "source": [
    "EBITDA對企業價值比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c9560cde-bb00-4cf2-9443-a7a039440b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asas9\\AppData\\Local\\Temp\\ipykernel_28296\\366518415.py:5: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Date  ticker  Total_SALEQ_4_Quarters_Rolling  \\\n",
      "268    2017-10-20  1101.0                    2.018721e+12   \n",
      "269    2017-10-23  1101.0                    2.018847e+12   \n",
      "270    2017-10-24  1101.0                    2.018973e+12   \n",
      "271    2017-10-25  1101.0                    2.019099e+12   \n",
      "272    2017-10-26  1101.0                    2.019225e+12   \n",
      "...           ...     ...                             ...   \n",
      "100266 2024-10-04  6505.0                    1.469524e+13   \n",
      "100267 2024-10-07  6505.0                    1.467818e+13   \n",
      "100268 2024-10-08  6505.0                    1.466113e+13   \n",
      "100269 2024-10-09  6505.0                    1.464408e+13   \n",
      "100271 2024-10-11  6505.0                    1.462703e+13   \n",
      "\n",
      "        Avg_EV_4_Quarters_Rolling   EBITDAEV  \n",
      "268                  2.007452e+11  10.056136  \n",
      "269                  2.006998e+11  10.059037  \n",
      "270                  2.006525e+11  10.062035  \n",
      "271                  2.006129e+11  10.064652  \n",
      "272                  2.005663e+11  10.067616  \n",
      "...                           ...        ...  \n",
      "100266               7.217637e+11  20.360178  \n",
      "100267               7.209090e+11  20.360663  \n",
      "100268               7.200792e+11  20.360444  \n",
      "100269               7.191663e+11  20.362579  \n",
      "100271               7.182349e+11  20.365244  \n",
      "\n",
      "[80387 rows x 5 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asas9\\AppData\\Local\\Temp\\ipykernel_28296\\366518415.py:22: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.set_index('Date').groupby('ticker', group_keys=False).apply(lambda x: x.asfreq('B')).reset_index()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the uploaded CSV file\n",
    "file_path = '50合併.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert Date column to datetime format and sort by date and ticker\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%Y/%m/%d')\n",
    "df = df.sort_values(by=['ticker', 'Date'])\n",
    "\n",
    "# Convert relevant columns to numeric types to avoid type errors\n",
    "df['SALEQ'] = pd.to_numeric(df['SALEQ'], errors='coerce')\n",
    "df['CSHOQ'] = pd.to_numeric(df['CSHOQ'], errors='coerce')\n",
    "df['Adj_Close'] = pd.to_numeric(df['Adj_Close'], errors='coerce')\n",
    "df['DLTTQ'] = pd.to_numeric(df['DLTTQ'], errors='coerce')\n",
    "df['DLCQ'] = pd.to_numeric(df['DLCQ'], errors='coerce')\n",
    "df['PSTKQ'] = pd.to_numeric(df['PSTKQ'], errors='coerce')\n",
    "df['MIBTQ'] = pd.to_numeric(df['MIBTQ'], errors='coerce')\n",
    "df['CHEQ'] = pd.to_numeric(df['CHEQ'], errors='coerce')\n",
    "\n",
    "# Ensure each group has a continuous business day index\n",
    "df = df.set_index('Date').groupby('ticker', group_keys=False).apply(lambda x: x.asfreq('B')).reset_index()\n",
    "\n",
    "# Calculate Enterprise Value (EV) based on the given formula\n",
    "df['EV'] = (df['CSHOQ'] * df['Adj_Close']) + df['DLTTQ'] + df['DLCQ'] + df['PSTKQ'] + df['MIBTQ'] - df['CHEQ']\n",
    "\n",
    "# Calculate rolling sum of SALEQ over the past 4 quarters (approx. 252 trading days)\n",
    "df['Total_SALEQ_4_Quarters_Rolling'] = df.groupby('ticker')['SALEQ'].rolling(window=252, min_periods=252).sum().reset_index(level=0, drop=True)\n",
    "\n",
    "# Calculate rolling average of EV over the past 4 quarters (approx. 252 trading days)\n",
    "df['Avg_EV_4_Quarters_Rolling'] = df.groupby('ticker')['EV'].rolling(window=252, min_periods=252).mean().reset_index(level=0, drop=True)\n",
    "\n",
    "# Calculate EBITDA-to-EV ratio (EBITDAEV) based on the provided formula\n",
    "df['EBITDAEV'] = df['Total_SALEQ_4_Quarters_Rolling'] / df['Avg_EV_4_Quarters_Rolling']\n",
    "\n",
    "# Filter the DataFrame to remove rows with missing values in the calculated columns\n",
    "df_filtered = df[['Date', 'ticker', 'Total_SALEQ_4_Quarters_Rolling', 'Avg_EV_4_Quarters_Rolling', 'EBITDAEV']].dropna()\n",
    "\n",
    "# Display the filtered results\n",
    "print(df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cd0518e5-1979-409f-84bd-3b7b84341447",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_columns = ['Date', 'ticker', 'EBITDAEV']\n",
    "# 导出到 CSV 文件\n",
    "df_filtered[export_columns].to_csv('EBITDAEV.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cc157a-e84a-4f65-96bb-dd0d19e2611b",
   "metadata": {},
   "source": [
    "自由現金流對市值比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d4ffbd7d-07a5-463a-8087-8b7adef584a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asas9\\AppData\\Local\\Temp\\ipykernel_28296\\2000640030.py:5: DtypeWarning: Columns (11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Date  ticker  Total_Free_Cash_Flow_4_Quarters_Rolling  \\\n",
      "268    2017-10-20  1101.0                             7.660719e+11   \n",
      "269    2017-10-23  1101.0                             7.650780e+11   \n",
      "270    2017-10-24  1101.0                             7.640840e+11   \n",
      "271    2017-10-25  1101.0                             7.630901e+11   \n",
      "272    2017-10-26  1101.0                             7.620961e+11   \n",
      "...           ...     ...                                      ...   \n",
      "100266 2024-10-04  6505.0                             1.297907e+12   \n",
      "100267 2024-10-07  6505.0                             1.290404e+12   \n",
      "100268 2024-10-08  6505.0                             1.282901e+12   \n",
      "100269 2024-10-09  6505.0                             1.275399e+12   \n",
      "100271 2024-10-11  6505.0                             1.267896e+12   \n",
      "\n",
      "        Avg_CSHOQ_4_Quarters_Rolling  Adj_Close       FCFP  \n",
      "268                     3.692176e+09      18.41  11.270246  \n",
      "269                     3.692176e+09      18.35  11.292426  \n",
      "270                     3.692176e+09      18.22  11.358222  \n",
      "271                     3.692176e+09      18.19  11.362155  \n",
      "272                     3.692176e+09      18.16  11.366101  \n",
      "...                              ...        ...        ...  \n",
      "100266                  9.525960e+09      54.50   2.499990  \n",
      "100267                  9.525960e+09      55.00   2.462943  \n",
      "100268                  9.525960e+09      54.20   2.484764  \n",
      "100269                  9.525960e+09      52.30   2.559974  \n",
      "100271                  9.525960e+09      52.00   2.559596  \n",
      "\n",
      "[81484 rows x 6 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asas9\\AppData\\Local\\Temp\\ipykernel_28296\\2000640030.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.set_index('Date').groupby('ticker', group_keys=False).apply(lambda x: x.asfreq('B')).reset_index()\n",
      "C:\\Users\\asas9\\AppData\\Local\\Temp\\ipykernel_28296\\2000640030.py:22: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df['Total_Free_Cash_Flow_4_Quarters_Rolling'] = df.groupby('ticker').apply(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the uploaded CSV file\n",
    "file_path = '50合併.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert Date column to datetime format and sort by date and ticker\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%Y/%m/%d')\n",
    "df = df.sort_values(by=['ticker', 'Date'])\n",
    "\n",
    "# Convert relevant columns to numeric types to avoid type errors\n",
    "df['OANCF'] = pd.to_numeric(df['OANCF'], errors='coerce')\n",
    "df['CAPXQ'] = pd.to_numeric(df['CAPXQ'], errors='coerce')\n",
    "df['DVQ'] = pd.to_numeric(df['DVQ'], errors='coerce')\n",
    "df['CSHOQ'] = pd.to_numeric(df['CSHOQ'], errors='coerce')\n",
    "df['Adj_Close'] = pd.to_numeric(df['Adj_Close'], errors='coerce')\n",
    "\n",
    "# Ensure each group has a continuous business day index\n",
    "df = df.set_index('Date').groupby('ticker', group_keys=False).apply(lambda x: x.asfreq('B')).reset_index()\n",
    "\n",
    "# Calculate rolling sum of (OANCF - CAPXQ - DVQ) over the past 4 quarters (approx. 252 trading days)\n",
    "df['Total_Free_Cash_Flow_4_Quarters_Rolling'] = df.groupby('ticker').apply(\n",
    "    lambda x: (x['OANCF'] - x['CAPXQ'] - x['DVQ']).rolling(window=252, min_periods=252).sum()\n",
    ").reset_index(level=0, drop=True)\n",
    "\n",
    "# Calculate rolling average of CSHOQ over the past 4 quarters (approx. 252 trading days)\n",
    "df['Avg_CSHOQ_4_Quarters_Rolling'] = df.groupby('ticker')['CSHOQ'].rolling(window=252, min_periods=252).mean().reset_index(level=0, drop=True)\n",
    "\n",
    "# Calculate Free Cash Flow to Price ratio (FCFP) based on the provided formula\n",
    "df['FCFP'] = df['Total_Free_Cash_Flow_4_Quarters_Rolling'] / (df['Adj_Close'] * df['Avg_CSHOQ_4_Quarters_Rolling'])\n",
    "\n",
    "# Filter the DataFrame to remove rows with missing values in the calculated columns\n",
    "df_filtered = df[['Date', 'ticker', 'Total_Free_Cash_Flow_4_Quarters_Rolling', 'Avg_CSHOQ_4_Quarters_Rolling', 'Adj_Close', 'FCFP']].dropna()\n",
    "\n",
    "# Display the filtered results\n",
    "print(df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b740d6a1-e0be-450e-8e76-60c4fd1cb7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_columns = ['Date', 'ticker', 'FCFP']\n",
    "# 导出到 CSV 文件\n",
    "df_filtered[export_columns].to_csv('FCFP.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb6174e-fc69-4be0-8d6d-fd4022fd09ab",
   "metadata": {},
   "source": [
    " 市淨率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e11f0983-a21a-4750-a643-2340a99c32ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asas9\\AppData\\Local\\Temp\\ipykernel_28296\\1715699233.py:5: DtypeWarning: Columns (2,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Date  ticker          CEQQ         CSHOQ  Adj_Close        BP\n",
      "0      2016-10-11  1101.0  1.467727e+11  3.692176e+09      18.79  2.115613\n",
      "1      2016-10-12  1101.0  1.467727e+11  3.692176e+09      18.79  2.115613\n",
      "2      2016-10-13  1101.0  1.467727e+11  3.692176e+09      18.24  2.179406\n",
      "3      2016-10-14  1101.0  1.467727e+11  3.692176e+09      18.68  2.128071\n",
      "4      2016-10-17  1101.0  1.467727e+11  3.692176e+09      18.79  2.115613\n",
      "...           ...     ...           ...           ...        ...       ...\n",
      "100266 2024-10-04  6505.0  3.321784e+11  9.525960e+09      54.50  0.639832\n",
      "100267 2024-10-07  6505.0  3.321784e+11  9.525960e+09      55.00  0.634016\n",
      "100268 2024-10-08  6505.0  3.321784e+11  9.525960e+09      54.20  0.643374\n",
      "100269 2024-10-09  6505.0  3.321784e+11  9.525960e+09      52.30  0.666747\n",
      "100271 2024-10-11  6505.0  3.321784e+11  9.525960e+09      52.00  0.670594\n",
      "\n",
      "[93514 rows x 6 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asas9\\AppData\\Local\\Temp\\ipykernel_28296\\1715699233.py:17: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.set_index('Date').groupby('ticker', group_keys=False).apply(lambda x: x.asfreq('B')).reset_index()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the uploaded CSV file\n",
    "file_path = '50合併.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert Date column to datetime format and sort by date and ticker\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%Y/%m/%d')\n",
    "df = df.sort_values(by=['ticker', 'Date'])\n",
    "\n",
    "# Convert relevant columns to numeric types to avoid type errors\n",
    "df['CEQQ'] = pd.to_numeric(df['CEQQ'], errors='coerce')\n",
    "df['CSHOQ'] = pd.to_numeric(df['CSHOQ'], errors='coerce')\n",
    "df['Adj_Close'] = pd.to_numeric(df['Adj_Close'], errors='coerce')\n",
    "\n",
    "# Ensure each group has a continuous business day index\n",
    "df = df.set_index('Date').groupby('ticker', group_keys=False).apply(lambda x: x.asfreq('B')).reset_index()\n",
    "\n",
    "# Calculate Book-to-Price ratio (BP) based on the provided formula\n",
    "df['BP'] = df['CEQQ'] / (df['CSHOQ'] * df['Adj_Close'])\n",
    "\n",
    "# Filter the DataFrame to remove rows with missing values in the calculated columns\n",
    "df_filtered = df[['Date', 'ticker', 'CEQQ', 'CSHOQ', 'Adj_Close', 'BP']].dropna()\n",
    "\n",
    "# Display the filtered results\n",
    "print(df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "928efaf4-18c2-47bf-b4e9-48df61048f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_columns = ['Date', 'ticker', 'BP']\n",
    "# 导出到 CSV 文件\n",
    "df_filtered[export_columns].to_csv('BP.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6329cc20-a37b-4d91-864e-aa86ac1033bb",
   "metadata": {},
   "source": [
    "市值的對數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8e5e82de-befc-4652-80c4-2e0ff1f199a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asas9\\AppData\\Local\\Temp\\ipykernel_28296\\2239441258.py:6: DtypeWarning: Columns (2,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Date  ticker         CSHOQ  Adj_Close          MCap   log_MCap\n",
      "0      2016-10-11  1101.0  3.692176e+09      18.79  6.937599e+10  24.962807\n",
      "1      2016-10-12  1101.0  3.692176e+09      18.79  6.937599e+10  24.962807\n",
      "2      2016-10-13  1101.0  3.692176e+09      18.24  6.734529e+10  24.933099\n",
      "3      2016-10-14  1101.0  3.692176e+09      18.68  6.896985e+10  24.956935\n",
      "4      2016-10-17  1101.0  3.692176e+09      18.79  6.937599e+10  24.962807\n",
      "...           ...     ...           ...        ...           ...        ...\n",
      "100266 2024-10-04  6505.0  9.525960e+09      54.50  5.191648e+11  26.975487\n",
      "100267 2024-10-07  6505.0  9.525960e+09      55.00  5.239278e+11  26.984620\n",
      "100268 2024-10-08  6505.0  9.525960e+09      54.20  5.163070e+11  26.969967\n",
      "100269 2024-10-09  6505.0  9.525960e+09      52.30  4.982077e+11  26.934283\n",
      "100271 2024-10-11  6505.0  9.525960e+09      52.00  4.953499e+11  26.928530\n",
      "\n",
      "[93514 rows x 6 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asas9\\AppData\\Local\\Temp\\ipykernel_28296\\2239441258.py:17: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.set_index('Date').groupby('ticker', group_keys=False).apply(lambda x: x.asfreq('B')).reset_index()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the uploaded CSV file\n",
    "file_path = '50合併.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert Date column to datetime format and sort by date and ticker\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%Y/%m/%d')\n",
    "df = df.sort_values(by=['ticker', 'Date'])\n",
    "\n",
    "# Convert relevant columns to numeric types to avoid type errors\n",
    "df['CSHOQ'] = pd.to_numeric(df['CSHOQ'], errors='coerce')\n",
    "df['Adj_Close'] = pd.to_numeric(df['Adj_Close'], errors='coerce')\n",
    "\n",
    "# Ensure each group has a continuous business day index\n",
    "df = df.set_index('Date').groupby('ticker', group_keys=False).apply(lambda x: x.asfreq('B')).reset_index()\n",
    "\n",
    "# Calculate Market Capitalization (MCap) and its logarithm based on the provided formula\n",
    "# Calculate market capitalization\n",
    "df['MCap'] = df['CSHOQ'] * df['Adj_Close']\n",
    "\n",
    "# Calculate the natural logarithm of the market capitalization\n",
    "df['log_MCap'] = np.log(df['MCap'])\n",
    "\n",
    "# Filter the DataFrame to remove rows with missing values in the calculated columns\n",
    "df_filtered_mcap = df[['Date', 'ticker', 'CSHOQ', 'Adj_Close', 'MCap', 'log_MCap']].dropna()\n",
    "\n",
    "# Display the filtered results\n",
    "print(df_filtered_mcap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f5faad25-12be-4604-af3b-8f3f317f42a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_columns = ['Date', 'ticker', 'log_MCap']\n",
    "# 导出到 CSV 文件\n",
    "df_filtered_mcap[export_columns].to_csv('log_MCap.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64c97db-2e87-4c9d-836e-3f993c6b5a5c",
   "metadata": {},
   "source": [
    "銷售額的對數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "02bb085f-a168-46e8-b981-756934a4d19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asas9\\AppData\\Local\\Temp\\ipykernel_28296\\1375968686.py:6: DtypeWarning: Columns (2,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Date  ticker     TTM_Sales  log_TTMSales\n",
      "268    2017-10-20  1101.0  2.018721e+12     28.333485\n",
      "269    2017-10-23  1101.0  2.018847e+12     28.333548\n",
      "270    2017-10-24  1101.0  2.018973e+12     28.333610\n",
      "271    2017-10-25  1101.0  2.019099e+12     28.333672\n",
      "272    2017-10-26  1101.0  2.019225e+12     28.333735\n",
      "...           ...     ...           ...           ...\n",
      "100266 2024-10-04  6505.0  1.469524e+13     30.318545\n",
      "100267 2024-10-07  6505.0  1.467818e+13     30.317383\n",
      "100268 2024-10-08  6505.0  1.466113e+13     30.316221\n",
      "100269 2024-10-09  6505.0  1.464408e+13     30.315057\n",
      "100271 2024-10-11  6505.0  1.462703e+13     30.313892\n",
      "\n",
      "[81504 rows x 4 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asas9\\AppData\\Local\\Temp\\ipykernel_28296\\1375968686.py:16: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.set_index('Date').groupby('ticker', group_keys=False).apply(lambda x: x.asfreq('B')).reset_index()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the uploaded CSV file\n",
    "file_path = '50合併.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert Date column to datetime format and sort by date and ticker\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%Y/%m/%d')\n",
    "df = df.sort_values(by=['ticker', 'Date'])\n",
    "\n",
    "# Convert relevant columns to numeric types to avoid type errors\n",
    "df['SALEQ'] = pd.to_numeric(df['SALEQ'], errors='coerce')\n",
    "\n",
    "# Ensure each group has a continuous business day index\n",
    "df = df.set_index('Date').groupby('ticker', group_keys=False).apply(lambda x: x.asfreq('B')).reset_index()\n",
    "\n",
    "# Calculate rolling sum of SALESQ over the past 4 quarters (approx. 252 trading days)\n",
    "df['TTM_Sales'] = df.groupby('ticker')['SALEQ'].rolling(window=252, min_periods=252).sum().reset_index(level=0, drop=True)\n",
    "\n",
    "# Calculate the natural logarithm of the TTM Sales\n",
    "df['log_TTMSales'] = np.log(df['TTM_Sales'])\n",
    "\n",
    "# Filter the DataFrame to remove rows with missing values in the calculated columns for TTM Sales\n",
    "df_filtered_ttmsales = df[['Date', 'ticker', 'TTM_Sales', 'log_TTMSales']].dropna()\n",
    "\n",
    "# Display the filtered results for TTM Sales\n",
    "print(df_filtered_ttmsales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "931e8f5d-4575-428e-85ea-6ef77e8869b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_columns = ['Date', 'ticker', 'log_TTMSales']\n",
    "# 导出到 CSV 文件\n",
    "df_filtered_ttmsales[export_columns].to_csv('log_TTMSales.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b4d88a-8ef8-4144-8311-48579477adf3",
   "metadata": {},
   "source": [
    "5日價格反轉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "789a92ec-ee4b-4daa-a230-c2c2d7ffeee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asas9\\AppData\\Local\\Temp\\ipykernel_28296\\2403878929.py:5: DtypeWarning: Columns (2,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Date  ticker  Adj_Close      PM5D\n",
      "5      2016-10-18  1101.0      18.97 -0.009580\n",
      "6      2016-10-19  1101.0      19.46 -0.035657\n",
      "7      2016-10-20  1101.0      19.23 -0.054276\n",
      "8      2016-10-21  1101.0      19.13 -0.024090\n",
      "9      2016-10-24  1101.0      19.28 -0.026078\n",
      "...           ...     ...        ...       ...\n",
      "100266 2024-10-04  6505.0      54.50 -0.064453\n",
      "100267 2024-10-07  6505.0      55.00 -0.084813\n",
      "100268 2024-10-08  6505.0      54.20 -0.022642\n",
      "100269 2024-10-09  6505.0      52.30  0.007590\n",
      "100271 2024-10-11  6505.0      52.00  0.005736\n",
      "\n",
      "[93249 rows x 4 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asas9\\AppData\\Local\\Temp\\ipykernel_28296\\2403878929.py:15: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.set_index('Date').groupby('ticker', group_keys=False).apply(lambda x: x.asfreq('B')).reset_index()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the uploaded CSV file\n",
    "file_path = '50合併.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert Date column to datetime format and sort by date and ticker\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%Y/%m/%d')\n",
    "df = df.sort_values(by=['ticker', 'Date'])\n",
    "\n",
    "# Convert relevant columns to numeric types to avoid type errors\n",
    "df['Adj_Close'] = pd.to_numeric(df['Adj_Close'], errors='coerce')\n",
    "\n",
    "# Ensure each group has a continuous business day index\n",
    "df = df.set_index('Date').groupby('ticker', group_keys=False).apply(lambda x: x.asfreq('B')).reset_index()\n",
    "\n",
    "# Calculate 5-day Price Reversal (PM5D)\n",
    "df['PM5D'] = (df.groupby('ticker')['Adj_Close'].shift(5) - df['Adj_Close']) / df.groupby('ticker')['Adj_Close'].shift(5)\n",
    "\n",
    "# Filter the DataFrame to remove rows with missing values in the calculated columns for PM5D\n",
    "df_filtered_pm5d = df[['Date', 'ticker', 'Adj_Close', 'PM5D']].dropna()\n",
    "\n",
    "# Display the filtered results for PM5D\n",
    "print(df_filtered_pm5d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "882deae3-7d7f-4950-8e69-19def3e68a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_columns = ['Date', 'ticker', 'PM5D']\n",
    "# 导出到 CSV 文件\n",
    "df_filtered_pm5d[export_columns].to_csv('PM5D.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4bebf6-424c-4a75-b44f-1f8b60a14140",
   "metadata": {},
   "outputs": [],
   "source": [
    " 9個月價格動能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "024603e8-6c46-477d-9dbb-d564bb3ec179",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asas9\\AppData\\Local\\Temp\\ipykernel_28296\\287458747.py:5: DtypeWarning: Columns (2,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Date  ticker  Adj_Close      PM9M\n",
      "203    2017-07-21  1101.0      18.81  0.001064\n",
      "204    2017-07-24  1101.0      18.76 -0.001597\n",
      "205    2017-07-25  1101.0      18.76  0.028509\n",
      "206    2017-07-26  1101.0      18.73  0.002677\n",
      "207    2017-07-27  1101.0      19.11  0.017030\n",
      "...           ...     ...        ...       ...\n",
      "100266 2024-10-04  6505.0      54.50 -0.321971\n",
      "100267 2024-10-07  6505.0      55.00 -0.307392\n",
      "100268 2024-10-08  6505.0      54.20 -0.302984\n",
      "100269 2024-10-09  6505.0      52.30 -0.331544\n",
      "100271 2024-10-11  6505.0      52.00 -0.344345\n",
      "\n",
      "[84413 rows x 4 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asas9\\AppData\\Local\\Temp\\ipykernel_28296\\287458747.py:15: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.set_index('Date').groupby('ticker', group_keys=False).apply(lambda x: x.asfreq('B')).reset_index()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the uploaded CSV file\n",
    "file_path = '50合併.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert Date column to datetime format and sort by date and ticker\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%Y/%m/%d')\n",
    "df = df.sort_values(by=['ticker', 'Date'])\n",
    "\n",
    "# Convert relevant columns to numeric types to avoid type errors\n",
    "df['Adj_Close'] = pd.to_numeric(df['Adj_Close'], errors='coerce')\n",
    "\n",
    "# Ensure each group has a continuous business day index\n",
    "df = df.set_index('Date').groupby('ticker', group_keys=False).apply(lambda x: x.asfreq('B')).reset_index()\n",
    "\n",
    "# Calculate 9-month Price Momentum (PM9M) using the adjusted close prices\n",
    "# Shift by approximately 9 months (assuming 21 trading days per month, so 189 trading days)\n",
    "df['PM9M'] = (df['Adj_Close'] - df.groupby('ticker')['Adj_Close'].shift(189)) / df.groupby('ticker')['Adj_Close'].shift(189)\n",
    "\n",
    "# Filter the DataFrame to remove rows with missing values in the calculated columns for PM9M\n",
    "df_filtered_pm9m = df[['Date', 'ticker', 'Adj_Close', 'PM9M']].dropna()\n",
    "\n",
    "# Display the filtered results for PM9M\n",
    "print(df_filtered_pm9m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a801d5ed-d0b8-4a6b-880f-c94fef9a38a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_columns = ['Date', 'ticker', 'PM9M']\n",
    "# 导出到 CSV 文件\n",
    "df_filtered_pm9m[export_columns].to_csv('PM9M.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7504887c-d5cc-4061-959b-13f617d70e9b",
   "metadata": {},
   "source": [
    "1個月高低波動率比值 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "144733ce-d548-450e-9f0b-675f2b63c2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asas9\\AppData\\Local\\Temp\\ipykernel_28296\\1740645994.py:6: DtypeWarning: Columns (2,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Date  ticker  Adj_Close  HIGHM   LOWM      HL1M\n",
      "0     2016-11-09  1101.0      18.92  19.73  18.24  1.191176\n",
      "1     2016-11-10  1101.0      19.88  19.73  18.24 -0.091463\n",
      "2     2016-11-11  1101.0      19.54  19.88  18.24  0.261538\n",
      "3     2016-11-14  1101.0      19.31  19.88  18.68  0.904762\n",
      "4     2016-11-15  1101.0      19.13  19.88  18.71  1.785714\n",
      "...          ...     ...        ...    ...    ...       ...\n",
      "92401 2024-10-04  6505.0      54.50  55.10  47.40  0.084507\n",
      "92402 2024-10-07  6505.0      55.00  54.80  47.40 -0.026316\n",
      "92403 2024-10-08  6505.0      54.20  55.00  47.40  0.117647\n",
      "92404 2024-10-09  6505.0      52.30  55.00  47.40  0.551020\n",
      "92405 2024-10-11  6505.0      52.00  55.00  47.40  0.652174\n",
      "\n",
      "[92401 rows x 6 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asas9\\AppData\\Local\\Temp\\ipykernel_28296\\1740645994.py:16: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.set_index('Date').groupby('ticker', group_keys=False).apply(lambda x: x.asfreq('B')).reset_index()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the uploaded CSV file\n",
    "file_path = '50合併.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert Date column to datetime format and sort by date and ticker\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%Y/%m/%d')\n",
    "df = df.sort_values(by=['ticker', 'Date'])\n",
    "\n",
    "# Convert relevant columns to numeric types to avoid type errors\n",
    "df['Adj_Close'] = pd.to_numeric(df['Adj_Close'], errors='coerce')\n",
    "\n",
    "# Ensure each group has a continuous business day index\n",
    "df = df.set_index('Date').groupby('ticker', group_keys=False).apply(lambda x: x.asfreq('B')).reset_index()\n",
    "\n",
    "# Calculate rolling 1-month high and low (approx. 21 trading days), excluding the current day for each ticker\n",
    "df['HIGHM'] = df.groupby('ticker', group_keys=False)['Adj_Close'].apply(lambda x: x.shift(1).rolling(window=21, min_periods=21).max())\n",
    "df['LOWM'] = df.groupby('ticker', group_keys=False)['Adj_Close'].apply(lambda x: x.shift(1).rolling(window=21, min_periods=21).min())\n",
    "\n",
    "# Drop rows where HIGHM or LOWM are NaN due to insufficient past data\n",
    "df = df.dropna(subset=['HIGHM', 'LOWM']).reset_index(drop=True)\n",
    "\n",
    "# Calculate HL1M based on rolling high and low values, ensuring calculation only for valid rows\n",
    "df['HL1M'] = (df['HIGHM'] - df['Adj_Close']) / (df['Adj_Close'] - df['LOWM'])\n",
    "\n",
    "# Filter the DataFrame to remove rows with missing values in the calculated columns for HL1M\n",
    "df_filtered_hl1m = df[['Date', 'ticker', 'Adj_Close', 'HIGHM', 'LOWM', 'HL1M']].dropna()\n",
    "\n",
    "# Display the filtered results for HL1M\n",
    "df_filtered_hl1m = df_filtered_hl1m.sort_values(by=['ticker', 'Date'])\n",
    "print(df_filtered_hl1m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7a48820e-9e98-42d0-a617-55c9b0eac83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_columns = ['Date', 'ticker', 'HL1M']\n",
    "# 导出到 CSV 文件\n",
    "df_filtered_hl1m[export_columns].to_csv('HL1M.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9ff8a3-e7aa-480e-b8bb-14d948801c03",
   "metadata": {},
   "source": [
    "1個月價格反轉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c8e44a14-eaf5-4ac1-af07-44d06da12ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asas9\\AppData\\Local\\Temp\\ipykernel_28296\\2455823385.py:6: DtypeWarning: Columns (2,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Date  ticker  Adj_Close  Current_Month_Close  \\\n",
      "41     2016-12-07  1101.0      19.31            19.223333   \n",
      "42     2016-12-08  1101.0      19.80            19.212381   \n",
      "43     2016-12-09  1101.0      19.78            19.254286   \n",
      "44     2016-12-12  1101.0      19.75            19.249524   \n",
      "45     2016-12-13  1101.0      19.73            19.259524   \n",
      "...           ...     ...        ...                  ...   \n",
      "100266 2024-10-04  6505.0      54.50            51.102381   \n",
      "100267 2024-10-07  6505.0      55.00            51.073810   \n",
      "100268 2024-10-08  6505.0      54.20            51.083333   \n",
      "100269 2024-10-09  6505.0      52.30            51.207143   \n",
      "100271 2024-10-11  6505.0      52.00            51.302381   \n",
      "\n",
      "        Previous_Month_Close      PM1M  \n",
      "41                 19.150952  0.003779  \n",
      "42                 19.157143  0.002883  \n",
      "43                 19.209048  0.002355  \n",
      "44                 19.270952 -0.001112  \n",
      "45                 19.300952 -0.002146  \n",
      "...                      ...       ...  \n",
      "100266             59.914286 -0.147075  \n",
      "100267             59.561905 -0.142509  \n",
      "100268             58.923810 -0.133061  \n",
      "100269             58.271429 -0.121231  \n",
      "100271             57.714286 -0.111097  \n",
      "\n",
      "[91341 rows x 6 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asas9\\AppData\\Local\\Temp\\ipykernel_28296\\2455823385.py:16: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('ticker').apply(lambda x: x.set_index('Date').asfreq('B').reset_index()).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the uploaded CSV file\n",
    "file_path = '50合併.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert Date column to datetime format and sort by date and ticker\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%Y/%m/%d')\n",
    "df = df.sort_values(by=['ticker', 'Date'])\n",
    "\n",
    "# Convert relevant columns to numeric types to avoid type errors\n",
    "df['Adj_Close'] = pd.to_numeric(df['Adj_Close'], errors='coerce')\n",
    "\n",
    "# Ensure each group has a continuous business day index, preserving individual date ranges for each ticker\n",
    "df = df.groupby('ticker').apply(lambda x: x.set_index('Date').asfreq('B').reset_index()).reset_index(drop=True)\n",
    "\n",
    "# Calculate adjusted close prices for the current month (days 22 to 42) and previous month (days 1 to 21)\n",
    "df['Previous_Month_Close'] = df.groupby('ticker', group_keys=False)['Adj_Close'].apply(lambda x: x.shift(21).rolling(window=21, min_periods=21).mean())\n",
    "df['Current_Month_Close'] = df.groupby('ticker', group_keys=False)['Adj_Close'].apply(lambda x: x.shift(1).rolling(window=21, min_periods=21).mean())\n",
    "\n",
    "# Calculate 1-month Price Reversal (PM1M) on a daily rolling basis starting from day 42\n",
    "df['PM1M'] = (df['Current_Month_Close'] - df['Previous_Month_Close']) / df['Previous_Month_Close']\n",
    "\n",
    "# Filter the DataFrame to remove rows with missing values in the calculated columns for PM1M\n",
    "df_filtered_pm1m = df[['Date', 'ticker', 'Adj_Close', 'Current_Month_Close', 'Previous_Month_Close', 'PM1M']].dropna()\n",
    "\n",
    "# Display the filtered results for PM1M\n",
    "df_filtered_pm1m = df_filtered_pm1m.sort_values(by=['ticker', 'Date'])\n",
    "print(df_filtered_pm1m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a98fe306-7a5b-4386-a9ab-01284610e86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_columns = ['Date', 'ticker', 'PM1M']\n",
    "# 导出到 CSV 文件\n",
    "df_filtered_pm1m[export_columns].to_csv('PM1M.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db9f197-81d2-41c2-bd79-6fcfe90734ea",
   "metadata": {},
   "source": [
    "12個月減去1個月的動能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "efc7771d-eb24-4eb5-bd75-740686c74774",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asas9\\AppData\\Local\\Temp\\ipykernel_28296\\4069550168.py:6: DtypeWarning: Columns (2,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Date  ticker  Adj_Close  Previous_Month_Close  \\\n",
      "269    2017-10-23  1101.0      18.35                 18.44   \n",
      "270    2017-10-24  1101.0      18.22                 18.41   \n",
      "271    2017-10-25  1101.0      18.19                 18.35   \n",
      "272    2017-10-26  1101.0      18.16                 18.41   \n",
      "273    2017-10-27  1101.0      18.16                 18.49   \n",
      "...           ...     ...        ...                   ...   \n",
      "100266 2024-10-04  6505.0      54.50                 55.10   \n",
      "100267 2024-10-07  6505.0      55.00                 54.80   \n",
      "100268 2024-10-08  6505.0      54.20                 51.60   \n",
      "100269 2024-10-09  6505.0      52.30                 50.30   \n",
      "100271 2024-10-11  6505.0      52.00                 50.90   \n",
      "\n",
      "        Twelve_Month_Close   PM12M1M  \n",
      "269                  18.79 -0.018627  \n",
      "270                  18.79 -0.020224  \n",
      "271                  18.24  0.006031  \n",
      "272                  18.68 -0.014454  \n",
      "273                  18.79 -0.015966  \n",
      "...                    ...       ...  \n",
      "100266               78.73 -0.300140  \n",
      "100267               79.02 -0.306505  \n",
      "100268               77.56 -0.334709  \n",
      "100269               77.86 -0.353969  \n",
      "100271               78.05 -0.347854  \n",
      "\n",
      "[81378 rows x 6 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asas9\\AppData\\Local\\Temp\\ipykernel_28296\\4069550168.py:16: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('ticker').apply(lambda x: x.set_index('Date').asfreq('B').reset_index()).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the uploaded CSV file\n",
    "file_path = '50合併.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert Date column to datetime format and sort by date and ticker\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%Y/%m/%d')\n",
    "df = df.sort_values(by=['ticker', 'Date'])\n",
    "\n",
    "# Convert relevant columns to numeric types to avoid type errors\n",
    "df['Adj_Close'] = pd.to_numeric(df['Adj_Close'], errors='coerce')\n",
    "\n",
    "# Ensure each group has a continuous business day index, preserving individual date ranges for each ticker\n",
    "df = df.groupby('ticker').apply(lambda x: x.set_index('Date').asfreq('B').reset_index()).reset_index(drop=True)\n",
    "\n",
    "# Calculate adjusted close prices for the previous month (shifted by 1 month) and 12 months ago\n",
    "# Previous month (shifted by 21 trading days)\n",
    "df['Previous_Month_Close'] = df.groupby('ticker', group_keys=False)['Adj_Close'].apply(lambda x: x.shift(21))\n",
    "# 12 months ago (shifted by 252 trading days, approx. 12 months)\n",
    "df['Twelve_Month_Close'] = df.groupby('ticker', group_keys=False)['Adj_Close'].apply(lambda x: x.shift(252))\n",
    "\n",
    "# Calculate 12M - 1M Price Momentum (PM12M1M) on a daily rolling basis\n",
    "df['PM12M1M'] = (df['Previous_Month_Close'] - df['Twelve_Month_Close']) / df['Twelve_Month_Close']\n",
    "\n",
    "# Filter the DataFrame to remove rows with missing values in the calculated columns for PM12M1M\n",
    "df_filtered_pm12m1m = df[['Date', 'ticker', 'Adj_Close', 'Previous_Month_Close', 'Twelve_Month_Close', 'PM12M1M']].dropna()\n",
    "\n",
    "# Display the filtered results for PM12M1M\n",
    "df_filtered_pm12m1m = df_filtered_pm12m1m.sort_values(by=['ticker', 'Date'])\n",
    "print(df_filtered_pm12m1m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f6cbec22-5479-4ef6-89d7-57141ab745ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_columns = ['Date', 'ticker', 'PM12M1M']\n",
    "# 导出到 CSV 文件\n",
    "df_filtered_pm12m1m[export_columns].to_csv('PM12M1M.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481f3b09-c3b4-49bc-ae62-89382f79be3f",
   "metadata": {},
   "source": [
    "**可持續增長率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0ea5c1a3-0be1-4646-8cd3-874c4298597e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asas9\\AppData\\Local\\Temp\\ipykernel_7732\\2348592915.py:6: DtypeWarning: Columns (2,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Date  ticker   EPS  DVQ          CEQQ         CSHOQ  TTM_EPS  \\\n",
      "269    2017-10-23  1101.0  0.58  0.0  1.450000e+11  3.692176e+09   125.13   \n",
      "270    2017-10-24  1101.0  0.58  0.0  1.450000e+11  3.692176e+09   125.17   \n",
      "271    2017-10-25  1101.0  0.58  0.0  1.450000e+11  3.692176e+09   125.21   \n",
      "272    2017-10-26  1101.0  0.58  0.0  1.450000e+11  3.692176e+09   125.25   \n",
      "273    2017-10-27  1101.0  0.58  0.0  1.450000e+11  3.692176e+09   125.29   \n",
      "...           ...     ...   ...  ...           ...           ...      ...   \n",
      "100266 2024-10-04  6505.0  0.29  0.0  3.320000e+11  9.525960e+09   204.63   \n",
      "100267 2024-10-07  6505.0  0.29  0.0  3.320000e+11  9.525960e+09   205.04   \n",
      "100268 2024-10-08  6505.0  0.29  0.0  3.320000e+11  9.525960e+09   205.45   \n",
      "100269 2024-10-09  6505.0  0.29  0.0  3.320000e+11  9.525960e+09   205.86   \n",
      "100271 2024-10-11  6505.0  0.29  0.0  3.320000e+11  9.525960e+09   206.27   \n",
      "\n",
      "        TTM_DVQ  RetentionRatio     TTMCEPS  SusGrwRate  \n",
      "269         0.0             1.0  158.348575  158.348575  \n",
      "270         0.0             1.0  158.339977  158.339977  \n",
      "271         0.0             1.0  158.331379  158.331379  \n",
      "272         0.0             1.0  158.322781  158.322781  \n",
      "273         0.0             1.0  158.314183  158.314183  \n",
      "...         ...             ...         ...         ...  \n",
      "100266      0.0             1.0  139.021947  139.021947  \n",
      "100267      0.0             1.0  139.043608  139.043608  \n",
      "100268      0.0             1.0  139.065270  139.065270  \n",
      "100269      0.0             1.0  139.086932  139.086932  \n",
      "100271      0.0             1.0  139.108594  139.108594  \n",
      "\n",
      "[80088 rows x 11 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asas9\\AppData\\Local\\Temp\\ipykernel_7732\\2348592915.py:19: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.set_index('Date').groupby('ticker', group_keys=False).apply(lambda x: x.asfreq('B')).reset_index()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the uploaded CSV file\n",
    "file_path = '50合併.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert Date column to datetime format and sort by date and ticker\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%Y/%m/%d')\n",
    "df = df.sort_values(by=['ticker', 'Date'])\n",
    "\n",
    "# Convert relevant columns to numeric types to avoid type errors\n",
    "df['EPS'] = pd.to_numeric(df['EPS'], errors='coerce')\n",
    "df['DVQ'] = pd.to_numeric(df['DVQ'], errors='coerce')\n",
    "df['CEQQ'] = pd.to_numeric(df['CEQQ'], errors='coerce')\n",
    "df['CSHOQ'] = pd.to_numeric(df['CSHOQ'], errors='coerce')\n",
    "\n",
    "# Ensure each group has a continuous business day index, preserving individual date ranges for each ticker\n",
    "df = df.set_index('Date').groupby('ticker', group_keys=False).apply(lambda x: x.asfreq('B')).reset_index()\n",
    "\n",
    "# Calculate Retention Ratio based on the past 4 quarters (approx. 252 trading days)\n",
    "df['TTM_EPS'] = df.groupby('ticker', group_keys=False)['EPS'].apply(lambda x: x.shift(1).rolling(window=252, min_periods=252).sum())\n",
    "df['TTM_DVQ'] = df.groupby('ticker', group_keys=False)['DVQ'].apply(lambda x: x.shift(1).rolling(window=252, min_periods=252).sum())\n",
    "df['RetentionRatio'] = 1 - (df['TTM_DVQ'] / df['TTM_EPS'])\n",
    "\n",
    "# Calculate TTMCEPS (Trailing Twelve Month Common Equity per Share) based on the past 4 quarters\n",
    "df['TTM_CEQQ'] = df.groupby('ticker', group_keys=False)['CEQQ'].apply(lambda x: x.shift(1).rolling(window=252, min_periods=252).sum())\n",
    "df['TTM_CSHOQ'] = df.groupby('ticker', group_keys=False)['CSHOQ'].apply(lambda x: x.shift(1).rolling(window=252, min_periods=252).sum())\n",
    "df['TTMCEPS'] = df['TTM_CEQQ'] / (df['TTM_CSHOQ'] / 4)\n",
    "\n",
    "# Calculate Sustainable Growth Rate (SusGrwRate)\n",
    "df['SusGrwRate'] = df['RetentionRatio'] * df['TTMCEPS']\n",
    "\n",
    "# Filter the DataFrame to remove rows with missing values in the calculated columns\n",
    "df_filtered_susgrwrate = df[['Date', 'ticker', 'EPS', 'DVQ', 'CEQQ', 'CSHOQ', 'TTM_EPS', 'TTM_DVQ', 'RetentionRatio', 'TTMCEPS', 'SusGrwRate']].dropna()\n",
    "\n",
    "# Display the filtered results for Sustainable Growth Rate\n",
    "df_filtered_susgrwrate = df_filtered_susgrwrate.sort_values(by=['ticker', 'Date'])\n",
    "print(df_filtered_susgrwrate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "46c89c84-153a-47fa-9651-0ab43c2914db",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_columns = ['Date', 'ticker', 'SusGrwRate']\n",
    "# 导出到 CSV 文件\n",
    "df_filtered_susgrwrate[export_columns].to_csv('SusGrwRate.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acaba13-d618-445c-ac8a-a1ce138f5346",
   "metadata": {},
   "source": [
    "EPS 成長率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "713b6aed-4919-49ba-9f32-2b760aed4b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asas9\\AppData\\Local\\Temp\\ipykernel_18500\\46015242.py:6: DtypeWarning: Columns (2,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Date  ticker  Adj_Close  TTMEPS  TTMEPS_1Y_ago  Chg1YEPS\n",
      "536    2018-10-31  1101.0      21.44  202.38         125.13  0.617358\n",
      "537    2018-11-01  1101.0      22.03  203.31         125.17  0.624271\n",
      "538    2018-11-02  1101.0      22.52  204.24         125.21  0.631180\n",
      "539    2018-11-05  1101.0      21.84  205.17         125.25  0.638084\n",
      "540    2018-11-06  1101.0      21.69  206.10         125.29  0.644984\n",
      "...           ...     ...        ...     ...            ...       ...\n",
      "100266 2024-10-04  6505.0      54.50  204.63          27.08  6.556499\n",
      "100267 2024-10-07  6505.0      55.00  205.04          25.18  7.142971\n",
      "100268 2024-10-08  6505.0      54.20  205.45          23.28  7.825172\n",
      "100269 2024-10-09  6505.0      52.30  205.86          21.38  8.628625\n",
      "100271 2024-10-11  6505.0      52.00  206.27          19.48  9.588809\n",
      "\n",
      "[67978 rows x 6 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asas9\\AppData\\Local\\Temp\\ipykernel_18500\\46015242.py:17: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.set_index('Date').groupby('ticker', group_keys=False).apply(lambda x: x.asfreq('B')).reset_index()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the uploaded CSV file\n",
    "file_path = '50合併.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert Date column to datetime format and sort by date and ticker\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%Y/%m/%d')\n",
    "df = df.sort_values(by=['ticker', 'Date'])\n",
    "\n",
    "# Convert relevant columns to numeric types to avoid type errors\n",
    "df['Adj_Close'] = pd.to_numeric(df['Adj_Close'], errors='coerce')\n",
    "df['EPS'] = pd.to_numeric(df['EPS'], errors='coerce')\n",
    "\n",
    "# Ensure each group has a continuous business day index, preserving individual date ranges for each ticker\n",
    "df = df.set_index('Date').groupby('ticker', group_keys=False).apply(lambda x: x.asfreq('B')).reset_index()\n",
    "\n",
    "# Calculate TTMEPS (Trailing Twelve Months Earnings Per Share) based on the past 4 quarters (approx. 252 trading days)\n",
    "df['TTMEPS'] = df.groupby('ticker', group_keys=False)['EPS'].apply(lambda x: x.shift(1).rolling(window=252, min_periods=252).sum())\n",
    "\n",
    "# Calculate TTMEPS from one year ago (approx. 252 trading days)\n",
    "df['TTMEPS_1Y_ago'] = df.groupby('ticker', group_keys=False)['TTMEPS'].shift(252)\n",
    "\n",
    "# Calculate Change in 1-Year EPS (Chg1YEPS)\n",
    "df['Chg1YEPS'] = (df['TTMEPS'] - df['TTMEPS_1Y_ago']) / df['TTMEPS_1Y_ago']\n",
    "\n",
    "# Filter the DataFrame to remove rows with missing values in the calculated columns\n",
    "df_filtered_chg1yeps = df[['Date', 'ticker', 'Adj_Close', 'TTMEPS', 'TTMEPS_1Y_ago', 'Chg1YEPS']].dropna()\n",
    "\n",
    "# Display the filtered results for Change in 1-Year EPS\n",
    "df_filtered_chg1yeps = df_filtered_chg1yeps.sort_values(by=['ticker', 'Date'])\n",
    "print(df_filtered_chg1yeps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41cf2abc-3be9-4a26-9129-e317184aaee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_columns = ['Date', 'ticker', 'Chg1YEPS']\n",
    "# 导出到 CSV 文件\n",
    "df_filtered_chg1yeps[export_columns].to_csv('Chg1YEPS.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0868f8ef-129a-44b6-bd90-b9f0ccc1f403",
   "metadata": {},
   "source": [
    "資產周轉率變動"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8985407c-4add-402e-8754-3a60d7a20392",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asas9\\AppData\\Local\\Temp\\ipykernel_18500\\115106120.py:6: DtypeWarning: Columns (2,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Date  ticker         SALEQ           ATQ      TTMSALES  \\\n",
      "536    2018-10-31  1101.0  1.175330e+10  3.099022e+11  2.579530e+12   \n",
      "537    2018-11-01  1101.0  1.175330e+10  3.099022e+11  2.582896e+12   \n",
      "538    2018-11-02  1101.0  1.175330e+10  3.099022e+11  2.586261e+12   \n",
      "539    2018-11-05  1101.0  1.175330e+10  3.099022e+11  2.589627e+12   \n",
      "540    2018-11-06  1101.0  1.175330e+10  3.099022e+11  2.592993e+12   \n",
      "...           ...     ...           ...           ...           ...   \n",
      "100266 2024-10-04  6505.0  5.332454e+10  4.286200e+11  1.471229e+13   \n",
      "100267 2024-10-07  6505.0  5.332454e+10  4.286200e+11  1.469524e+13   \n",
      "100268 2024-10-08  6505.0  5.332454e+10  4.286200e+11  1.467818e+13   \n",
      "100269 2024-10-09  6505.0  5.332454e+10  4.286200e+11  1.466113e+13   \n",
      "100271 2024-10-11  6505.0  5.332454e+10  4.286200e+11  1.464408e+13   \n",
      "\n",
      "              AvgAst  TTMSALES_1Y_ago  AvgAst_1Y_ago  Chg1YAstTo  \n",
      "536     2.814545e+11     2.018721e+12   2.637138e+11    1.510030  \n",
      "537     2.816440e+11     2.018847e+12   2.636875e+11    1.514573  \n",
      "538     2.818335e+11     2.018973e+12   2.636613e+11    1.519109  \n",
      "539     2.820229e+11     2.019099e+12   2.636350e+11    1.523637  \n",
      "540     2.822124e+11     2.019225e+12   2.636087e+11    1.528157  \n",
      "...              ...              ...            ...         ...  \n",
      "100266  4.215467e+11     1.550919e+13   4.345797e+11   -0.787066  \n",
      "100267  4.216459e+11     1.549682e+13   4.342785e+11   -0.831988  \n",
      "100268  4.217451e+11     1.548445e+13   4.339773e+11   -0.876882  \n",
      "100269  4.218443e+11     1.547208e+13   4.336760e+11   -0.921748  \n",
      "100271  4.219435e+11     1.545971e+13   4.333748e+11   -0.966586  \n",
      "\n",
      "[69360 rows x 9 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asas9\\AppData\\Local\\Temp\\ipykernel_18500\\115106120.py:17: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.set_index('Date').groupby('ticker', group_keys=False).apply(lambda x: x.asfreq('B')).reset_index()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the uploaded CSV file\n",
    "file_path = '50合併.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert Date column to datetime format and sort by date and ticker\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%Y/%m/%d')\n",
    "df = df.sort_values(by=['ticker', 'Date'])\n",
    "\n",
    "# Convert relevant columns to numeric types to avoid type errors\n",
    "df['SALEQ'] = pd.to_numeric(df['SALEQ'], errors='coerce')\n",
    "df['ATQ'] = pd.to_numeric(df['ATQ'], errors='coerce')\n",
    "\n",
    "# Ensure each group has a continuous business day index, preserving individual date ranges for each ticker\n",
    "df = df.set_index('Date').groupby('ticker', group_keys=False).apply(lambda x: x.asfreq('B')).reset_index()\n",
    "\n",
    "# Calculate TTMSALES (Trailing Twelve Months Sales) based on the past 4 quarters (approx. 252 trading days)\n",
    "df['TTMSALES'] = df.groupby('ticker', group_keys=False)['SALEQ'].apply(lambda x: x.shift(1).rolling(window=252, min_periods=252).sum())\n",
    "\n",
    "# Calculate AvgAst (Average Total Assets) based on the past 4 quarters (approx. 252 trading days)\n",
    "df['AvgAst'] = df.groupby('ticker', group_keys=False)['ATQ'].apply(lambda x: x.shift(1).rolling(window=252, min_periods=252).mean())\n",
    "\n",
    "# Calculate TTMSALES and AvgAst from one year ago (approx. 252 trading days)\n",
    "df['TTMSALES_1Y_ago'] = df.groupby('ticker', group_keys=False)['TTMSALES'].shift(252)\n",
    "df['AvgAst_1Y_ago'] = df.groupby('ticker', group_keys=False)['AvgAst'].shift(252)\n",
    "\n",
    "# Calculate Change in 1-Year Asset Turnover (Chg1YAstTo)\n",
    "df['Chg1YAstTo'] = (df['TTMSALES'] / df['AvgAst']) - (df['TTMSALES_1Y_ago'] / df['AvgAst_1Y_ago'])\n",
    "\n",
    "# Filter the DataFrame to remove rows with missing values in the calculated columns\n",
    "df_filtered_chg1yastto = df[['Date', 'ticker', 'SALEQ', 'ATQ', 'TTMSALES', 'AvgAst', 'TTMSALES_1Y_ago', 'AvgAst_1Y_ago', 'Chg1YAstTo']].dropna()\n",
    "\n",
    "# Display the filtered results for Change in 1-Year Asset Turnover\n",
    "df_filtered_chg1yastto = df_filtered_chg1yastto.sort_values(by=['ticker', 'Date'])\n",
    "print(df_filtered_chg1yastto)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fbff846-9b37-40de-bc7f-1e19de00707a",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_columns = ['Date', 'ticker', 'Chg1YAstTo']\n",
    "# 导出到 CSV 文件\n",
    "df_filtered_chg1yastto[export_columns].to_csv('Chg1YAstTo.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35aa596-986d-43d2-8cfc-d869b1441c8b",
   "metadata": {},
   "source": [
    "淨利潤率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46e53f04-b2b0-4fc3-8168-d15a0f2a1a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asas9\\AppData\\Local\\Temp\\ipykernel_7732\\3383108178.py:6: DtypeWarning: Columns (2,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Date  ticker         SALEQ           IBQ      TTMSales  \\\n",
      "269    2017-10-23  1101.0  8.387381e+09  4.079056e+09  2.018721e+12   \n",
      "270    2017-10-24  1101.0  8.387381e+09  4.079056e+09  2.018847e+12   \n",
      "271    2017-10-25  1101.0  8.387381e+09  4.079056e+09  2.018973e+12   \n",
      "272    2017-10-26  1101.0  8.387381e+09  4.079056e+09  2.019099e+12   \n",
      "273    2017-10-27  1101.0  8.387381e+09  4.079056e+09  2.019225e+12   \n",
      "...           ...     ...           ...           ...           ...   \n",
      "100266 2024-10-04  6505.0  5.332454e+10  3.260798e+09  1.471229e+13   \n",
      "100267 2024-10-07  6505.0  5.332454e+10  3.260798e+09  1.469524e+13   \n",
      "100268 2024-10-08  6505.0  5.332454e+10  3.260798e+09  1.467818e+13   \n",
      "100269 2024-10-09  6505.0  5.332454e+10  3.260798e+09  1.466113e+13   \n",
      "100271 2024-10-11  6505.0  5.332454e+10  3.260798e+09  1.464408e+13   \n",
      "\n",
      "               TTMIB  NetProfitMargin  \n",
      "269     8.475890e+11         0.419864  \n",
      "270     8.481998e+11         0.420141  \n",
      "271     8.488107e+11         0.420417  \n",
      "272     8.494216e+11         0.420693  \n",
      "273     8.500324e+11         0.420970  \n",
      "...              ...              ...  \n",
      "100266  2.322154e+12         0.157838  \n",
      "100267  2.327334e+12         0.158373  \n",
      "100268  2.332515e+12         0.158910  \n",
      "100269  2.337696e+12         0.159449  \n",
      "100271  2.342877e+12         0.159988  \n",
      "\n",
      "[81456 rows x 7 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asas9\\AppData\\Local\\Temp\\ipykernel_7732\\3383108178.py:17: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.set_index('Date').groupby('ticker', group_keys=False).apply(lambda x: x.asfreq('B')).reset_index()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the uploaded CSV file\n",
    "file_path = '50合併.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert Date column to datetime format and sort by date and ticker\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%Y/%m/%d')\n",
    "df = df.sort_values(by=['ticker', 'Date'])\n",
    "\n",
    "# Convert relevant columns to numeric types to avoid type errors\n",
    "df['SALEQ'] = pd.to_numeric(df['SALEQ'], errors='coerce')\n",
    "df['IBQ'] = pd.to_numeric(df['IBQ'], errors='coerce')\n",
    "\n",
    "# Ensure each group has a continuous business day index, preserving individual date ranges for each ticker\n",
    "df = df.set_index('Date').groupby('ticker', group_keys=False).apply(lambda x: x.asfreq('B')).reset_index()\n",
    "\n",
    "# Calculate TTM Sales (Trailing Twelve Months Sales) based on the past 4 quarters (approx. 252 trading days)\n",
    "df['TTMSales'] = df.groupby('ticker', group_keys=False)['SALEQ'].apply(lambda x: x.shift(1).rolling(window=252, min_periods=252).sum())\n",
    "\n",
    "# Calculate TTM Income Before Extraordinary Items (TTMIB) based on the past 4 quarters (approx. 252 trading days)\n",
    "df['TTMIB'] = df.groupby('ticker', group_keys=False)['IBQ'].apply(lambda x: x.shift(1).rolling(window=252, min_periods=252).sum())\n",
    "\n",
    "# Calculate Net Profit Margin (NetProfitMargin)\n",
    "df['NetProfitMargin'] = df['TTMIB'] / df['TTMSales']\n",
    "\n",
    "# Filter the DataFrame to remove rows with missing values in the calculated columns\n",
    "df_filtered_npm = df[['Date', 'ticker', 'SALEQ', 'IBQ', 'TTMSales', 'TTMIB', 'NetProfitMargin']].dropna()\n",
    "\n",
    "# Display the filtered results for Net Profit Margin\n",
    "df_filtered_npm = df_filtered_npm.sort_values(by=['ticker', 'Date'])\n",
    "print(df_filtered_npm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97bd3b39-3121-4ec6-beef-2a52990f4217",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_columns = ['Date', 'ticker', 'NetProfitMargin']\n",
    "# 导出到 CSV 文件\n",
    "df_filtered_npm[export_columns].to_csv('NetProfitMargin.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab19cbc6-3048-4887-b754-912470551767",
   "metadata": {},
   "source": [
    "現金循環週期"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28af9d0b-0bad-441c-920a-bbaada024c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asas9\\AppData\\Local\\Temp\\ipykernel_7732\\599497380.py:6: DtypeWarning: Columns (2,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Date  ticker         SALEQ         RECTQ         INVTQ  \\\n",
      "269    2017-10-23  1101.0  2.471408e+10  2.065391e+10  9.360034e+09   \n",
      "270    2017-10-24  1101.0  2.471408e+10  2.065391e+10  9.360034e+09   \n",
      "271    2017-10-25  1101.0  2.471408e+10  2.065391e+10  9.360034e+09   \n",
      "272    2017-10-26  1101.0  2.471408e+10  2.065391e+10  9.360034e+09   \n",
      "273    2017-10-27  1101.0  2.471408e+10  2.065391e+10  9.360034e+09   \n",
      "...           ...     ...           ...           ...           ...   \n",
      "100266 2024-10-04  6505.0  1.706164e+11  5.608295e+10  9.504617e+10   \n",
      "100267 2024-10-07  6505.0  1.706164e+11  5.608295e+10  9.504617e+10   \n",
      "100268 2024-10-08  6505.0  1.706164e+11  5.608295e+10  9.504617e+10   \n",
      "100269 2024-10-09  6505.0  1.706164e+11  5.608295e+10  9.504617e+10   \n",
      "100271 2024-10-11  6505.0  1.706164e+11  5.608295e+10  9.504617e+10   \n",
      "\n",
      "               COGSQ           APQ  Receivables_Days  Inventory_Days  \\\n",
      "269     1.944605e+10  7.466214e+09        295.884006      171.540391   \n",
      "270     1.944605e+10  7.466214e+09        296.016699      171.564901   \n",
      "271     1.944605e+10  7.466214e+09        296.149310      171.589395   \n",
      "272     1.944605e+10  7.466214e+09        296.281841      171.613872   \n",
      "273     1.944605e+10  7.466214e+09        296.414291      171.638333   \n",
      "...              ...           ...               ...             ...   \n",
      "100266  1.665800e+11  2.109351e+10        117.089564      185.085132   \n",
      "100267  1.665800e+11  2.109351e+10        117.106641      185.202485   \n",
      "100268  1.665800e+11  2.109351e+10        117.123708      185.319818   \n",
      "100269  1.665800e+11  2.109351e+10        117.140768      185.437132   \n",
      "100271  1.665800e+11  2.109351e+10        117.157818      185.554426   \n",
      "\n",
      "        Payables_Days   CashCycle  \n",
      "269        141.994617  325.429780  \n",
      "270        141.998787  325.582813  \n",
      "271        142.002954  325.735751  \n",
      "272        142.007119  325.888595  \n",
      "273        142.011280  326.041343  \n",
      "...               ...         ...  \n",
      "100266      48.780813  253.393883  \n",
      "100267      48.806689  253.502436  \n",
      "100268      48.832561  253.610966  \n",
      "100269      48.858428  253.719471  \n",
      "100271      48.884291  253.827953  \n",
      "\n",
      "[59395 rows x 11 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asas9\\AppData\\Local\\Temp\\ipykernel_7732\\599497380.py:20: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.set_index('Date').groupby('ticker', group_keys=False).apply(lambda x: x.asfreq('B')).reset_index()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the uploaded CSV file\n",
    "file_path = '50合併.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert Date column to datetime format and sort by date and ticker\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%Y/%m/%d')\n",
    "df = df.sort_values(by=['ticker', 'Date'])\n",
    "\n",
    "# Convert relevant columns to numeric types to avoid type errors\n",
    "df['SALEQ'] = pd.to_numeric(df['SALEQ'], errors='coerce')\n",
    "df['RECTQ'] = pd.to_numeric(df['RECTQ'], errors='coerce')\n",
    "df['INVTQ'] = pd.to_numeric(df['INVTQ'], errors='coerce')\n",
    "df['COGSQ'] = pd.to_numeric(df['COGSQ'], errors='coerce')\n",
    "df['APQ'] = pd.to_numeric(df['APQ'], errors='coerce')\n",
    "\n",
    "# Ensure each group has a continuous business day index, preserving individual date ranges for each ticker\n",
    "df = df.set_index('Date').groupby('ticker', group_keys=False).apply(lambda x: x.asfreq('B')).reset_index()\n",
    "\n",
    "# Calculate TTM Receivables Turnover (avg RECTQ / avg SALEQ)\n",
    "df['Avg_RECTQ'] = df.groupby('ticker', group_keys=False)['RECTQ'].apply(lambda x: x.shift(1).rolling(window=252, min_periods=252).mean())\n",
    "df['Avg_SALEQ'] = df.groupby('ticker', group_keys=False)['SALEQ'].apply(lambda x: x.shift(1).rolling(window=252, min_periods=252).mean())\n",
    "df['Receivables_Days'] = (df['Avg_RECTQ'] / df['Avg_SALEQ']) * 365\n",
    "\n",
    "# Calculate TTM Inventory Turnover (avg INVTQ / avg COGSQ)\n",
    "df['Avg_INVTQ'] = df.groupby('ticker', group_keys=False)['INVTQ'].apply(lambda x: x.shift(1).rolling(window=252, min_periods=252).mean())\n",
    "df['Avg_COGSQ'] = df.groupby('ticker', group_keys=False)['COGSQ'].apply(lambda x: x.shift(1).rolling(window=252, min_periods=252).mean())\n",
    "df['Inventory_Days'] = (df['Avg_INVTQ'] / df['Avg_COGSQ']) * 365\n",
    "\n",
    "# Calculate TTM Payables Turnover (avg APQ / avg COGSQ)\n",
    "df['Avg_APQ'] = df.groupby('ticker', group_keys=False)['APQ'].apply(lambda x: x.shift(1).rolling(window=252, min_periods=252).mean())\n",
    "df['Payables_Days'] = (df['Avg_APQ'] / df['Avg_COGSQ']) * 365\n",
    "\n",
    "# Calculate Cash Conversion Cycle (CashCycle)\n",
    "df['CashCycle'] = df['Receivables_Days'] + df['Inventory_Days'] - df['Payables_Days']\n",
    "\n",
    "# Filter the DataFrame to remove rows with missing values in the calculated columns\n",
    "df_filtered_ccc = df[['Date', 'ticker', 'SALEQ', 'RECTQ', 'INVTQ', 'COGSQ', 'APQ', 'Receivables_Days', 'Inventory_Days', 'Payables_Days', 'CashCycle']].dropna()\n",
    "\n",
    "# Display the filtered results for Cash Conversion Cycle\n",
    "df_filtered_ccc = df_filtered_ccc.sort_values(by=['ticker', 'Date'])\n",
    "print(df_filtered_ccc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "390fe451-7909-4f94-acc5-00f41db093a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_columns = ['Date', 'ticker', 'CashCycle']\n",
    "# 导出到 CSV 文件\n",
    "df_filtered_ccc[export_columns].to_csv('CashCycle.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52994638-5db4-4375-b068-0913867115f2",
   "metadata": {},
   "source": [
    "普通股股數變動率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fe43066-965d-47e9-b441-1bb32271e932",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asas9\\AppData\\Local\\Temp\\ipykernel_7732\\4095078716.py:6: DtypeWarning: Columns (2,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Date  ticker         CSHOQ  CSHOQ_1Y_ago  ShareChg\n",
      "269    2017-10-23  1101.0  3.692176e+09  3.692176e+09       0.0\n",
      "270    2017-10-24  1101.0  3.692176e+09  3.692176e+09       0.0\n",
      "271    2017-10-25  1101.0  3.692176e+09  3.692176e+09       0.0\n",
      "272    2017-10-26  1101.0  3.692176e+09  3.692176e+09       0.0\n",
      "273    2017-10-27  1101.0  3.692176e+09  3.692176e+09       0.0\n",
      "...           ...     ...           ...           ...       ...\n",
      "100266 2024-10-04  6505.0  9.525960e+09  9.525960e+09       0.0\n",
      "100267 2024-10-07  6505.0  9.525960e+09  9.525960e+09       0.0\n",
      "100268 2024-10-08  6505.0  9.525960e+09  9.525960e+09       0.0\n",
      "100269 2024-10-09  6505.0  9.525960e+09  9.525960e+09       0.0\n",
      "100271 2024-10-11  6505.0  9.525960e+09  9.525960e+09       0.0\n",
      "\n",
      "[81456 rows x 5 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asas9\\AppData\\Local\\Temp\\ipykernel_7732\\4095078716.py:16: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.set_index('Date').groupby('ticker', group_keys=False).apply(lambda x: x.asfreq('B')).reset_index()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the uploaded CSV file\n",
    "file_path = '50合併.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert Date column to datetime format and sort by date and ticker\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%Y/%m/%d')\n",
    "df = df.sort_values(by=['ticker', 'Date'])\n",
    "\n",
    "# Convert relevant columns to numeric types to avoid type errors\n",
    "df['CSHOQ'] = pd.to_numeric(df['CSHOQ'], errors='coerce')\n",
    "\n",
    "# Ensure each group has a continuous business day index, preserving individual date ranges for each ticker\n",
    "df = df.set_index('Date').groupby('ticker', group_keys=False).apply(lambda x: x.asfreq('B')).reset_index()\n",
    "\n",
    "# Calculate Shares Outstanding Change (ShareChg) based on the past 4 quarters (approx. 252 trading days)\n",
    "df['CSHOQ_1Y_ago'] = df.groupby('ticker', group_keys=False)['CSHOQ'].shift(252)\n",
    "df['ShareChg'] = (df['CSHOQ'] / df['CSHOQ_1Y_ago']) - 1\n",
    "\n",
    "# Filter the DataFrame to remove rows with missing values in the calculated columns\n",
    "df_filtered_sharechg = df[['Date', 'ticker', 'CSHOQ', 'CSHOQ_1Y_ago', 'ShareChg']].dropna()\n",
    "\n",
    "# Display the filtered results for Shares Outstanding Change\n",
    "df_filtered_sharechg = df_filtered_sharechg.sort_values(by=['ticker', 'Date'])\n",
    "print(df_filtered_sharechg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d6129f4-79df-4588-bf44-9676d96ad15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_columns = ['Date', 'ticker', 'ShareChg']\n",
    "# 导出到 CSV 文件\n",
    "df_filtered_sharechg[export_columns].to_csv('ShareChg.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbb182b-db16-43a2-84da-5d6ffd023e95",
   "metadata": {},
   "source": [
    "***資本獲取比率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bf267ae-73af-4c12-bf83-d64d5708c63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asas9\\AppData\\Local\\Temp\\ipykernel_7732\\4290710806.py:6: DtypeWarning: Columns (2,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Date  ticker         OANCF  DVQ         CAPXQ  TTM_OANCFQ_DVQ  \\\n",
      "269    2017-10-23  1101.0  3.208683e+09  0.0  4.983240e+08    8.473255e+11   \n",
      "270    2017-10-24  1101.0  3.208683e+09  0.0  4.983240e+08    8.463709e+11   \n",
      "271    2017-10-25  1101.0  3.208683e+09  0.0  4.983240e+08    8.454164e+11   \n",
      "272    2017-10-26  1101.0  3.208683e+09  0.0  4.983240e+08    8.444618e+11   \n",
      "273    2017-10-27  1101.0  3.208683e+09  0.0  4.983240e+08    8.435072e+11   \n",
      "...           ...     ...           ...  ...           ...             ...   \n",
      "100266 2024-10-04  6505.0 -9.599554e+09  0.0  1.572511e+09    1.728100e+12   \n",
      "100267 2024-10-07  6505.0 -9.599554e+09  0.0  1.572511e+09    1.719867e+12   \n",
      "100268 2024-10-08  6505.0 -9.599554e+09  0.0  1.572511e+09    1.711634e+12   \n",
      "100269 2024-10-09  6505.0 -9.599554e+09  0.0  1.572511e+09    1.703401e+12   \n",
      "100271 2024-10-11  6505.0 -9.599554e+09  0.0  1.572511e+09    1.695168e+12   \n",
      "\n",
      "           TTM_CAPXQ  CapAcqRatio  \n",
      "269     8.125352e+10    10.428169  \n",
      "270     8.129292e+10    10.411373  \n",
      "271     8.133232e+10    10.394593  \n",
      "272     8.137172e+10    10.377829  \n",
      "273     8.141112e+10    10.361081  \n",
      "...              ...          ...  \n",
      "100266  4.226906e+11     4.088334  \n",
      "100267  4.219603e+11     4.075898  \n",
      "100268  4.212300e+11     4.063419  \n",
      "100269  4.204997e+11     4.050897  \n",
      "100271  4.197694e+11     4.038331  \n",
      "\n",
      "[81456 rows x 8 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asas9\\AppData\\Local\\Temp\\ipykernel_7732\\4290710806.py:18: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.set_index('Date').groupby('ticker', group_keys=False).apply(lambda x: x.asfreq('B')).reset_index()\n",
      "C:\\Users\\asas9\\AppData\\Local\\Temp\\ipykernel_7732\\4290710806.py:21: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df['TTM_OANCFQ_DVQ'] = df.groupby('ticker', group_keys=False).apply(lambda x: (x['OANCF'] - x['DVQ']).shift(1).rolling(window=252, min_periods=252).sum())\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the uploaded CSV file\n",
    "file_path = '50合併.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Convert Date column to datetime format and sort by date and ticker\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%Y/%m/%d')\n",
    "df = df.sort_values(by=['ticker', 'Date'])\n",
    "\n",
    "# Convert relevant columns to numeric types to avoid type errors\n",
    "df['OANCF'] = pd.to_numeric(df['OANCF'], errors='coerce')\n",
    "df['DVQ'] = pd.to_numeric(df['DVQ'], errors='coerce')\n",
    "df['CAPXQ'] = pd.to_numeric(df['CAPXQ'], errors='coerce')\n",
    "\n",
    "# Ensure each group has a continuous business day index, preserving individual date ranges for each ticker\n",
    "df = df.set_index('Date').groupby('ticker', group_keys=False).apply(lambda x: x.asfreq('B')).reset_index()\n",
    "\n",
    "# Calculate Capital Acquisition Ratio (CapAcqRatio) based on the past 4 quarters (approx. 252 trading days)\n",
    "df['TTM_OANCFQ_DVQ'] = df.groupby('ticker', group_keys=False).apply(lambda x: (x['OANCF'] - x['DVQ']).shift(1).rolling(window=252, min_periods=252).sum())\n",
    "df['TTM_CAPXQ'] = df.groupby('ticker', group_keys=False)['CAPXQ'].apply(lambda x: x.shift(1).rolling(window=252, min_periods=252).sum())\n",
    "df['CapAcqRatio'] = df['TTM_OANCFQ_DVQ'] / df['TTM_CAPXQ']\n",
    "\n",
    "# Filter the DataFrame to remove rows with missing values in the calculated columns\n",
    "df_filtered_capacqratio = df[['Date', 'ticker', 'OANCF', 'DVQ', 'CAPXQ', 'TTM_OANCFQ_DVQ', 'TTM_CAPXQ', 'CapAcqRatio']].dropna()\n",
    "\n",
    "# Display the filtered results for Capital Acquisition Ratio\n",
    "df_filtered_capacqratio = df_filtered_capacqratio.sort_values(by=['ticker', 'Date'])\n",
    "print(df_filtered_capacqratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d52094-3d99-46d6-b53b-e801373e4f58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
